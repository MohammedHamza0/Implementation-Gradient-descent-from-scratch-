# Gradient Descent Algorithms for Linear Regression
This repository contains implementations of three variants of gradient descent algorithms for linear regression: Batch Gradient Descent (BGD), Mini-Batch Gradient Descent (MGD), and Stochastic Gradient Descent (SGD). These algorithms are used to optimize the parameters (thetas) of a linear regression model.

# Introduction
Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts the model parameters to find the minimum value of the cost function. This repository includes implementations of:

Batch Gradient Descent (BGD)
Mini-Batch Gradient Descent (MGD)
Stochastic Gradient Descent (SGD)

Each of these algorithms has its own characteristics and is suitable for different scenarios depending on the size of the dataset and the computational resources available.

# Features
Batch Gradient Descent (BGD): Updates the model parameters using the entire dataset.
Mini-Batch Gradient Descent (MGD): Updates the model parameters using small batches of the dataset.
Stochastic Gradient Descent (SGD): Updates the model parameters using a single data point at a time.

# Installation
To use the code in this repository, you need to have Python installed along with the following libraries:
NumPy
Pandas 
Matplotlib
sklearn
